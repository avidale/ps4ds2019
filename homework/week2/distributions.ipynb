{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"name":"distributions.ipynb","provenance":[],"collapsed_sections":["ODC260hRAA9n","BvCkMcESAA-B","uVfbGYELD6lM","9L1xvr8DAA_D","-WCSuW7oAA_K"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"JutCS2OCAA9d","colab_type":"text"},"source":["Contents of the notebook:\n","1. Look at a few pythonic examples of how you can analyze and visualize data.\n","1. Simulate a few regularities concerning convergence of sample means. \n","1. Make a few more experiments showing how different distributions arise.\n","1. Take look at the distribution of some financial data. \n","\n","You will have to just click throug these sections and play with them a little.\n","\n","And after that, there will be the actual work to do: describe the distribution of two more financial variables. \n"]},{"cell_type":"markdown","metadata":{"id":"z4ltze_sAA9h","colab_type":"text"},"source":["# 1. A few examples of what can you compute with data and distributions\n","\n","**What can you do with random variables**:\n","* calculate their cdf, quantiles, moments, etc from data - or from formulas\n","* visualize data samples and populations\n","* sample data from distribution formulas\n","* fit distribution formulas to data samples\n","\n","That's what we'll try here. We'l use the libraries `matplotlib`, `numpy`, `scipy` and `pandas`, which must be installed beforehand - e.g. using `pip` or `conda` package managers from command line. "]},{"cell_type":"code","metadata":{"id":"tEXA5n9oAA9j","colab_type":"code","colab":{}},"source":["import numpy as np\n","import scipy.stats\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ODC260hRAA9n","colab_type":"text"},"source":["## Calculating distribution properties"]},{"cell_type":"markdown","metadata":{"id":"4fOAIyfWAA9o","colab_type":"text"},"source":["#### This is how distribution properties can be calculated from data"]},{"cell_type":"code","metadata":{"id":"cg_DEuGWAA9p","colab_type":"code","colab":{}},"source":["data = [1, 2, 3, 4, 9, 10, 11, 17, 30, 49, 121, 289, 503]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gfwmoEMAA9s","colab_type":"code","colab":{}},"source":["# moments\n","print(np.mean(data))\n","print(np.std(data))\n","print(scipy.stats.skew(data))\n","print(scipy.stats.kurtosis(data))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RNteCocbAA9v","colab_type":"code","colab":{}},"source":["# linearly interpolated quantiles: min, 25% quantlie, median, 75% quantile, max\n","print(np.percentile(data, [0, 25, 50, 75, 100]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T3Q6SfKUAA9z","colab_type":"text"},"source":["#### And here these properties are calculated from population formulas\n","\n","Scipy has a special family of objects that represent distributions and have lots of interesting methods. \n","\n","See more details in https://docs.scipy.org/doc/scipy/reference/stats.html"]},{"cell_type":"code","metadata":{"id":"Ouc7q5HZAA90","colab_type":"code","colab":{}},"source":["# formula for a normal with mean 90 and standard deviation 100\n","distribution = scipy.stats.norm(loc=90, scale=100)\n","print(type(distribution))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_B7cuFuAA92","colab_type":"code","colab":{}},"source":["# analytical moments (someone took the integrals for you)\n","print(distribution.mean())\n","print(distribution.std())\n","print(distribution.stats('mvsk')) # mean, variance, skewness and kurtosis"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7H4ZaibSAA96","colab_type":"code","colab":{}},"source":["# cdf at different points\n","print(distribution.cdf([10, 100, 200]))\n","# cdf at different points\n","print(distribution.pdf([10, 100, 200]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rs_-sW70AA9-","colab_type":"code","colab":{}},"source":["# quantiles at different percentages\n","print(distribution.ppf([0.05, 0.5, 0.8, 0.95, 0.999]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BvCkMcESAA-B","colab_type":"text"},"source":["## Visualization"]},{"cell_type":"markdown","metadata":{"id":"yx56SoJkAA-C","colab_type":"text"},"source":["The most frequently used plot for distributions is histogram"]},{"cell_type":"code","metadata":{"id":"4MKT6mRPAA-D","colab_type":"code","colab":{}},"source":["plt.hist(data, bins=5);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zSSxpWvmAA-G","colab_type":"text"},"source":["Sometimes a histogram with unequal bins can be more informative - but don't forget to normalize it!"]},{"cell_type":"code","metadata":{"id":"YPkmhV16AA-H","colab_type":"code","colab":{}},"source":["bins = [0, 10, 100, 500]\n","plt.subplot(1,2,1)\n","plt.hist(data, bins=bins);\n","plt.subplot(1,2,2)\n","plt.hist(data, bins=bins, density=True);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MvwZyfa1AA-K","colab_type":"text"},"source":["To visualize (something like) the CDF of a dataset, you can plot the variable again its cumulative probability in a dataset."]},{"cell_type":"code","metadata":{"id":"iCJDUDieAA-L","colab_type":"code","colab":{}},"source":["plt.plot(sorted(data), np.linspace(0, 1, num=len(data)))\n","plt.xlabel('values of the random variable')\n","plt.ylabel('values of its CDF')\n","plt.title('linearly interpolated empirical CDF');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2h3bqjdXAA-O","colab_type":"text"},"source":["You can plot a kernel density estimate (with normal, a.k.a. gaussian kernel)."]},{"cell_type":"code","metadata":{"id":"M9djarCZAA-P","colab_type":"code","colab":{}},"source":["grid = np.linspace(0, 550)\n","bandwidths = [None, 0.2, 1]\n","for bw in bandwidths:\n","    kde = scipy.stats.kde.gaussian_kde(data, bw_method=bw)\n","    plt.plot(grid, kde.evaluate(grid));\n","bandwidths[0] = 'automatic'\n","plt.legend(bandwidths)\n","plt.title('Kernel density estimates with different kernel width');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lf84SRLrAA-S","colab_type":"text"},"source":["We can show how different methods of density estimation work with one more example."]},{"cell_type":"code","metadata":{"id":"wDigMVq2AA-T","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1,3,1)\n","\n","x = np.concatenate([\n","    np.random.normal(loc=1, scale=1, size=35), \n","    np.random.normal(loc=3, scale=3, size=15)\n","])\n","plt.hist(x, bins=20, density=True);\n","grid = np.linspace(x.min() - 1, x.max() + 1, 1000)\n","plt.plot(\n","    grid,\n","    scipy.stats.norm.pdf(grid, loc=1, scale=1)*0.7 \n","    + scipy.stats.norm.pdf(grid, loc=3, scale=3)*0.3\n",")\n","plt.legend(['population density', 'histogram'])\n","plt.title('Histogram density estimation');\n","\n","plt.subplot(1,3,2)\n","scales = [0.3,  3, 10]\n","for scale in scales:\n","    density = sum((np.abs(xi-grid) < scale * 0.5) / scale for xi in x) / len(x)\n","    plt.plot(grid, density);\n","plt.title('Moving window density estimate');\n","plt.legend(scales);\n","\n","plt.subplot(1,3,3)\n","scales = [0.1,  1, 5]\n","for scale in scales:\n","    density = sum(scipy.stats.norm(xi, scale=scale).pdf(grid) for xi in x) / len(x)\n","    plt.plot(grid, density);\n","plt.title('Normal kernel density estimate');\n","plt.legend(scales);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BTWeO5I4AA-W","colab_type":"text"},"source":["And one more example of looking at log scale for PDF and CDF"]},{"cell_type":"code","metadata":{"id":"seg2c3jpAA-X","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","pareto_x = scipy.stats.pareto.rvs(1.7, size=1000)\n","x_d = np.linspace(0, pareto_x.max() + 1, 1000)\n","density = scipy.stats.kde.gaussian_kde(pareto_x, bw_method=0.5).evaluate(x_d)\n","\n","plt.figure(figsize=(10, 6))\n","plt.subplot(2,3,1)\n","plt.hist(pareto_x, bins=30, density=True)\n","plt.title('Histogram')\n","\n","plt.subplot(2,3,2)\n","plt.plot(x_d, density)\n","plt.title('KDE')\n","\n","plt.subplot(2,3,3)\n","plt.plot(x_d, density)\n","plt.yscale('log')\n","plt.title('KDE, log scale')\n","\n","xs = np.array(sorted(pareto_x))\n","q = np.linspace(0, 1, num=len(pareto_x))\n","\n","plt.subplot(2,3,4)\n","plt.plot(xs, q)\n","plt.title('CDF')\n","\n","plt.subplot(2,3,5)\n","plt.plot(xs, 1-q)\n","plt.title('1 - CDF')\n","\n","plt.subplot(2,3,6)\n","plt.plot(xs, 1-q)\n","plt.yscale('log')\n","plt.xscale('log')\n","plt.title('1 - CDF, log-log scale')\n","\n","plt.tight_layout();"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PyOzZ3w4AA-b","colab_type":"text"},"source":["A brief way to plot several distribution is `boxplot` - it shows mean, 25% and 75% quantiles, range that contains most data (\"whiskers\"), and outliers."]},{"cell_type":"code","metadata":{"id":"JNot-Jj5AA-c","colab_type":"code","colab":{}},"source":["data1 = [1, 2, 4, 8, 12, 19, 23, 3, 54, 32, 1, 39]\n","data2 = [3, 5, 32, 54, 8, 32, 2, 4, 42]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7vQxcNOAA-g","colab_type":"code","colab":{}},"source":["plt.boxplot([data1, data2], whis=[5, 95]);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hlPrV6c5AA-j","colab_type":"text"},"source":["A violinplot shows roughly the same, but it just more fancy. "]},{"cell_type":"code","metadata":{"id":"kwbDvJk2AA-n","colab_type":"code","colab":{}},"source":["plt.violinplot([data1, data2]);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"thVdh-I1AA-w","colab_type":"text"},"source":["Bivariate data can be visualized as a scatterplot"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"S0B1QYRNAA-y","colab_type":"code","colab":{}},"source":["pairs1 = [1, 2, 3, 4, 2.5, 3.5]\n","pairs2 = [1, 3, 6, 4, 4, 2]\n","plt.scatter(pairs1, pairs2);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cd3CXCiCAA-2","colab_type":"text"},"source":["Or you can plot a \"genuine\" (from a population, instead of a dataset) PDFs or CDFs."]},{"cell_type":"code","metadata":{"id":"vSyjG0amAA-6","colab_type":"code","colab":{}},"source":["x = np.linspace(-3, 5, num=1000)\n","parameters = [[0, 1], [2, 1], [0, 0.5], [1, 2]]\n","plt.figure(figsize=(12,4))\n","plt.subplot(1,2,1)\n","for mu, sigma in parameters:\n","    plt.plot(x, scipy.stats.norm(mu, sigma).pdf(x))\n","plt.legend(parameters)\n","plt.title('Normal PDFs with different parameters')\n","plt.subplot(1,2,2)\n","for mu, sigma in parameters:\n","    plt.plot(x, scipy.stats.norm(mu, sigma).cdf(x))\n","plt.legend(parameters)\n","plt.title('Normal CDFs with different parameters');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2O8ON0JAA--","colab_type":"text"},"source":["Probability plot, a.k.a. quantile-quantlie plot, is a great way to see how well some distribution family fits to a dataset.\n","\n","If the fit is good, then most dots will lie close to the diagonal."]},{"cell_type":"code","metadata":{"id":"Fb7HSrCVAA-_","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(8, 4))\n","plt.subplot(1,2,1)\n","scipy.stats.probplot(pareto_x, dist=scipy.stats.norm, sparams=(pareto_x.mean(), pareto_x.std()), plot=plt)\n","plt.title('Q-Q plot, normal')\n","plt.subplot(1,2,2)\n","scipy.stats.probplot(pareto_x, dist=scipy.stats.pareto, sparams=(pareto_x.mean(), pareto_x.std()), plot=plt)\n","plt.title('Q-Q plot, Pareto');\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uVfbGYELD6lM","colab_type":"text"},"source":["## The assignment : visualize the distribution of word frequencies\n","\n","Let's calculate frequencies of all words from the previous homework. \n","\n","Your need to visualize the distribution of words frequencies (e.g. plot a histogram of them, or plot their CDF, in an appropriate scale). \n","\n","An additional question: from what family of probability distributions this data might have been generated?"]},{"cell_type":"code","metadata":{"id":"worCTEP5EMs5","colab_type":"code","colab":{}},"source":["%%capture\n","!wget https://raw.githubusercontent.com/avidale/ps4ds2019/master/homework/week1/spam_classifier/SMSSpamCollection"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5d4Ef3XENcJ","colab_type":"code","colab":{}},"source":["import re\n","import pandas as pd \n","from collections import Counter\n","data = pd.read_csv('SMSSpamCollection', sep='\\t', header=None)\n","data.columns = ['target', 'text']\n","words_frequencies = pd.Series(Counter(word for sent in data.text for word in re.split('\\W+', sent.lower()) if word))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TdF4HS9NFIP8","colab_type":"code","colab":{}},"source":["# todo: visualize the distribution of word frequencies "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o_l-nYkEAA_B","colab_type":"text"},"source":["# 2. Simulation of the Central Limit Theorem and Law of Large Numbers"]},{"cell_type":"markdown","metadata":{"id":"S1pRtfJ0AA_C","colab_type":"text"},"source":["The CLT states that sum of any ð‘› (approximately) independent and (approximately) identical random variables will converge to normal distribution, as we increase ð‘›."]},{"cell_type":"markdown","metadata":{"id":"9L1xvr8DAA_D","colab_type":"text"},"source":["### How CLT looks like"]},{"cell_type":"markdown","metadata":{"id":"qLgKkUY4AA_E","colab_type":"text"},"source":["You can take (almost) any distribution. The distribution may have any shape. "]},{"cell_type":"code","metadata":{"id":"Bb2FTfFMAA_F","colab_type":"code","colab":{}},"source":["np.random.seed(41)\n","samples = {'binomial':[], 'uniform':[], 'lognormal':[], 'exponential':[], 'pareto':[]}\n","ns = [1, 3, 10, 30, 100, 300, 1000, 3000]\n","for n in ns:\n","    samples['binomial'].append(np.random.binomial(n=1, p=0.5, size = (1000,n)).sum(axis = 1))\n","    samples['uniform'].append(np.random.uniform(size = (1000,n)).sum(axis = 1))\n","    samples['lognormal'].append(np.random.lognormal(0,1, size = (1000,n)).sum(axis = 1))\n","    samples['exponential'].append(np.random.exponential(0.1, size = (1000,n)).sum(axis = 1))\n","    samples['pareto'].append(np.random.pareto(3.5, size = (1000,n)).sum(axis = 1))\n","    \n","plt.figure(figsize=(12, 2))\n","for i, name in enumerate(samples):\n","    plt.subplot(1, 5, i+1)\n","    plt.hist(samples[name][0], bins = 20)\n","    plt.title(name)\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkZqoDLzAA_H","colab_type":"text"},"source":["But when you make a large sample from this distribution and calculate its average, it starts looking like normal. Independently of the original shape!"]},{"cell_type":"code","metadata":{"id":"cwQE5eeeAA_I","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(20, 12))\n","for j, n in enumerate(ns):\n","    for i, name in enumerate(samples):\n","        plt.subplot(5, len(ns), i*len(ns)+1+j)\n","        plt.hist(samples[name][j], bins = 20)\n","        plt.title('{}, n={}'.format(name, n))\n","plt.tight_layout()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-WCSuW7oAA_K","colab_type":"text"},"source":["### When CLT works\n","\n","In brief - when the underlying individual random variables have mean and variance themselves, then sampling sum converges to normal distribution.\n","\n","As an example, we can sample longer and longer sums from the exponential distribution. \n","\n","Let's track how the different sample moments converge and/or diverge when we add lots of random variables together."]},{"cell_type":"code","metadata":{"id":"DB5YZtfpAA_O","colab_type":"code","colab":{}},"source":["import numpy as np\n","import scipy.stats\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","def simulate_clt(distribution, size=100000, sample_sizes=(1, 2, 3, 10, 50, 100, 1000)):\n","    plt.figure(figsize=(14,2))\n","    np.random.seed(1)\n","    for i, n in enumerate(sample_sizes):\n","        sample_sum = distribution.rvs(size=(size, n)).sum(axis=1)\n","        plt.subplot(1, len(sample_sizes), i+1)\n","        plt.hist(sample_sum, bins=100);\n","        plt.yticks([])\n","        plt.title('n={}'.format(n))\n","        # TODO: calculate the first 4 moments for our random variable sample_sum\n","        # if you do it correctly, mean and variance in this example should diverge, and skewness and kurtosis - converge. \n","        mean = np.mean(sample_sum)\n","        std_deviation = np.std(sample_sum)\n","        skewness = scipy.stats.skew(sample_sum)\n","        excess_kurtosis = scipy.stats.kurtosis(sample_sum)\n","        print('n={:4}, mean={:8.3f}, std.dev={:6.3f}, skewness={:6.3f}, excess kurtosis={:6.3f}'.format(\n","            n, mean, std_deviation, skewness, excess_kurtosis))\n","    plt.subplots_adjust(top=0.75)\n","    \n","distribution = scipy.stats.expon()\n","simulate_clt(distribution)\n","plt.suptitle('Distribution of sum of n exponential variables');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pErC5o1UAA_R","colab_type":"text"},"source":["Another law of <s>nature</s> mathematics is the law of large numbers: it says that the mean of a large sample converges to the expected value of population from which the sample was taken.\n","\n","Let's try to simulate it as well!"]},{"cell_type":"code","metadata":{"id":"DC_v6UXlAA_R","colab_type":"code","colab":{}},"source":["def plot_sequential_sample(distribution, size=(500, 10000)):\n","    np.random.seed(1)\n","    sequential_sample = np.cumsum(distribution.rvs(size=size), axis=1)\n","    sequential_sample /= np.arange(sequential_sample.shape[1]) + 1\n","    plt.figure(figsize=(12,4))\n","    plt.subplot(1,2,1)\n","    plt.plot(sequential_sample.T, linewidth=0.2, color='k')\n","    plt.xlabel('sample size')\n","    plt.ylabel('sample mean')\n","    plt.subplot(1,2,2)\n","    plt.plot(sequential_sample.T, linewidth=0.2, color='k')\n","    plt.xlabel('sample size')\n","    plt.ylabel('sample mean')\n","    plt.xscale('log')\n","    if sequential_sample.min() > 0:\n","        plt.yscale('log')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxBUKf0jAA_U","colab_type":"text"},"source":["Start with the same exponential distribution"]},{"cell_type":"code","metadata":{"id":"BMqbMk7GAA_V","colab_type":"code","colab":{}},"source":["plot_sequential_sample(scipy.stats.expon(1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3KfPmSaZAA_Y","colab_type":"text"},"source":["However, with more capricious Pareto distribution (with low values of the shape parameter) the law of large numbers seems to fail: instead of convergence, more and more outliers emerge. \n","\n","Moreover, instead of stabilizing, sample means grow (and with overwhelming speed!) when we increase sample size. "]},{"cell_type":"code","metadata":{"id":"cd0uK7ANAA_Z","colab_type":"code","colab":{}},"source":["plot_sequential_sample(scipy.stats.pareto(b=0.3))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TRpLfzxVAA_b","colab_type":"text"},"source":["The central limit theorem as well doesn't seem to work with this distribution. "]},{"cell_type":"code","metadata":{"id":"nDk9NwqDAA_c","colab_type":"code","colab":{}},"source":["simulate_clt(scipy.stats.pareto(b=0.3))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEhBSel6IX91","colab_type":"text"},"source":["## The assignment\n","\n","For what parameters of Pareto distribution the Law of large numpers works, and for what parameters it does not?\n","\n","Make a guess and try to demonstrate its correctness."]},{"cell_type":"markdown","metadata":{"id":"LNq1zQxuIzZT","colab_type":"text"},"source":["** todo ** : your guess here"]},{"cell_type":"code","metadata":{"id":"tiEn_lPpIcWj","colab_type":"code","colab":{}},"source":["# todo: your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PtzFTqIdAA_e","colab_type":"text"},"source":["# 3. A few more random processes"]},{"cell_type":"markdown","metadata":{"id":"eeC_Hk3xAA_f","colab_type":"text"},"source":["This section just shows you how different distributions can arise from simple random processes. "]},{"cell_type":"markdown","metadata":{"id":"OJ_ap5uBAA_g","colab_type":"text"},"source":["### Reproduction of bacteria and lognormal distribution"]},{"cell_type":"markdown","metadata":{"id":"nHolvdz7AA_g","colab_type":"text"},"source":["Normal distribution emerges as a result of adding multiple similar variables. But sometimes in nature and in society we observe *multiplication* of random variables. In this case, resulting distribution may be lognormal. \n","\n","For example, let's consider a population of bacteria in a cup of broth. In favorable environments, they never die, and every bacterium every minute divides in two with probability 50%. In the first minute, there is only one bacterium. \n","\n","Q: What will be the distribution of number of bacteria in half an hour?\n","\n","A: It seems that it will be lognormal, because on average the number of bacteria is multiplied by 1.5 on every minute. "]},{"cell_type":"code","metadata":{"id":"WAQNtr7aAA_h","colab_type":"code","colab":{}},"source":["# let's write a function that generates bacteria!\n","def plot_germs(minutes=30, initial=1, p_divide=0.5):\n","    numbers_after_30_minutes = []\n","    # try it 10K times\n","    for i in range(10000):\n","        number_of_germs = initial\n","        for t in range(minutes):\n","            # all bacteria divide independently, so the number of successes (divisions) has binomial distribution\n","            number_of_germs += np.random.binomial(n=number_of_germs, p=p_divide)\n","        numbers_after_30_minutes.append(number_of_germs)\n","    h = plt.hist(numbers_after_30_minutes, bins=100, normed=True)\n","    return(h)\n","np.random.seed(42)\n","plot_germs(30);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0RXgXnWkAA_k","colab_type":"text"},"source":["In order to unserstand how the lognormal distribution emerges, let's look at the distribution after 2 minutes. "]},{"cell_type":"code","metadata":{"id":"QQzExUerAA_l","colab_type":"code","colab":{}},"source":["plot_germs(2);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Aht04v8AA_p","colab_type":"text"},"source":["What are the probabilities of having 1, 2, 3 or 4 bacteria after 2 minutes ?\n","\n","Yuo can find them analytically or just run the folliwing cell:"]},{"cell_type":"code","metadata":{"id":"H1lGCTswAA_q","colab_type":"code","colab":{}},"source":["distr = {}\n","distr[1] = 0.5*0.5 # in both minutes there are no divisions\n","distr[2] = 0.5*0.5 + 0.5*0.5*0.5 # there is a division in the first minute and no divisions in the second, or vice versa\n","distr[3] = 0.5*0.5*0.5 + 0.5*0.5*0.5 # one division in the first minute, and one (of 2 possible) in the second minute\n","distr[4] = 0.5*0.5*0.5 # in the first minute the first bacterium divides, in the second - both\n","print(distr) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qmfpetHJAA_u","colab_type":"text"},"source":["If you still don't see it, let's print all possible scenarios as a tree."]},{"cell_type":"code","metadata":{"id":"GezZNHZmAA_w","colab_type":"code","colab":{}},"source":["def print_probabilities(t=0, t_max=2, n=1, p=1):\n","    \"\"\"\n","    A recursive function that prints all possible reproduction scenarios\n","    \"\"\"\n","    print('{}Minute {}: {} bacteria, probability {}'.format('\\t'*t, t, n, p))\n","    if t < t_max:\n","        for n_new in range(0, n+1): # there can be from 0 to n new bacteria\n","            prob_n_new = scipy.stats.binom.pmf(n_new, n, 0.5)\n","            print_probabilities(t+1, t_max, n+n_new, p * prob_n_new)\n","print_probabilities(t_max = 2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-4mP0UBgAA_0","colab_type":"text"},"source":["Pay attention to the probabilities on the last level (minute 2). "]},{"cell_type":"markdown","metadata":{"id":"_XfENzEuAA_2","colab_type":"text"},"source":["### Lifecycle: the geometric/exponential distribution"]},{"cell_type":"markdown","metadata":{"id":"BmKaE5qJAA_3","colab_type":"text"},"source":["One more question that we can ask about this model is the following: in what age do the bacteria usually divide? How is it distributed?\n"]},{"cell_type":"markdown","metadata":{"id":"Ny6AsHMvAA_3","colab_type":"text"},"source":["Let's consider a \"newborn\" bacterium. With probability $1/2$ it divides on it first minute. With probability $1/4$ it does not divide on the first minute, but divides on the second minute. With probability $1/8$ it divides on the 3rd minute, and so on... So probabilities decrease in geometric progression, which gives the name to the distribution. \n","\n","Let's try to generate it. "]},{"cell_type":"code","metadata":{"id":"mVtcCNHFAA_4","colab_type":"code","colab":{}},"source":["# please run this cell for several times!\n","age = 0 # initially, the bacterium has age 0\n","while np.random.randint(2) == 0: # while a coin flips with 'tails', the bacterium does not divide ...\n","    age = age + 1 # ... but its age increases\n","print(age)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-oxtXOvAA_7","colab_type":"code","colab":{}},"source":["# let's repeat the experiment for 10K bacteria\n","ages = []\n","for t in range(10000):\n","    age = 0\n","    while np.random.randint(2) == 0: \n","        age = age + 1\n","    ages.append(age)\n","plt.hist(ages, bins = max(ages)+1)\n","plt.title('Sample from distribution of bacteria lifetime');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sf9APy2qAA_-","colab_type":"text"},"source":["## City sizes, internet memes, and Pareto law"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"7DMVaE3EAA_-","colab_type":"text"},"source":["Normal distribution is a result of adding independent variables, and it oftem emerges in systems with negative feedback or without feedback. But in society and economy there is often positive feedback. \n","\n","For example, the larger is the company budget, the more it can spend on advertizing, the more customers it has, the larger its budget... Or the more times a meme is reposted, the more eyes will see it, the more users will repost it... In general, the past reinforces the future, and this influence accumulates. "]},{"cell_type":"markdown","metadata":{"id":"gPUUj2sCAA__","colab_type":"text"},"source":["Let' simulate the process of reposting the memes. Let's start from the situation of no reposts. "]},{"cell_type":"code","metadata":{"id":"HgRw3nUaAA__","colab_type":"code","colab":{}},"source":["# 1000 memes, each one posted only once. \n","repost_counts = [1]\n","new_meme_probability = 0.01\n","plt.hist(repost_counts, bins=100);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5VimSZmjABAD","colab_type":"text"},"source":["Every new user chooses a meme randomly (with probability roughly proportional to the number of occurrences of this meme), and reposts it. "]},{"cell_type":"code","metadata":{"id":"Va55Z0l1ABAE","colab_type":"code","colab":{}},"source":["# add new users. Everyone reposts a random post or makes a new one, and its number of occurrences increases by 1. \n","for i in range(50000):\n","    if np.random.uniform() < new_meme_probability:\n","        repost_counts.append(1)\n","    else:\n","        proba = np.array(repost_counts) + 10.0 # an additional factor that makes chances more equal\n","        proba /=  sum(proba)\n","        chosen_post = np.random.choice(len(repost_counts), p=proba)\n","        repost_counts[chosen_post] += 1\n","plt.hist(repost_counts, bins=100);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HXiuAeo3ABAH","colab_type":"text"},"source":["After repeating the process multiple times, we see that there are a few 'leader' memes that leave all the competitors far behind. "]},{"cell_type":"markdown","metadata":{"id":"xa1vm9_lABAI","colab_type":"text"},"source":["The plot of ($1-CDF$) (and of $PDF$, if we knew it) for Pareto distribution is close to linear in log-log scale"]},{"cell_type":"code","metadata":{"id":"8bt-DNquABAK","colab_type":"code","colab":{}},"source":["plt.plot(\n","    1 - np.linspace(0, 1, len(repost_counts)),\n","    sorted(repost_counts)\n",")\n","plt.xscale('log')\n","plt.yscale('log')\n","plt.xlabel('x')\n","plt.ylabel('1-CDF');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IgGgBCTrABAM","colab_type":"text"},"source":["\n","The distributions of city sizes, personal incomes, and word occurences look pretty similar. "]},{"cell_type":"markdown","metadata":{"id":"X49XehiyABAN","colab_type":"text"},"source":["# 4. Stock Prices data: example of modelling a distribution"]},{"cell_type":"code","metadata":{"id":"_cY6HQmiABAN","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.stats\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p4JDZAt_ABAQ","colab_type":"text"},"source":["Here we download the data on the 500 top US public companies from a [website](https://datahub.io/core/s-and-p-500-companies-financials). "]},{"cell_type":"code","metadata":{"id":"Vv_P2NOXABAQ","colab_type":"code","colab":{}},"source":["url = 'https://datahub.io/core/s-and-p-500-companies-financials/r/constituents-financials.csv'\n","data = pd.read_csv(url)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v2ysjywDABAS","colab_type":"text"},"source":["Let's take a look at our data."]},{"cell_type":"code","metadata":{"id":"54deqU-LABAT","colab_type":"code","colab":{}},"source":["data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SfomkSIjABAX","colab_type":"text"},"source":["### Example of analysis of a variable\n","\n","Let's take stock price and see how it is distributed. \n","\n","Here we will use some of magical `pandas` methods. In the 3rd week, you'll study them in more details on the Python course. "]},{"cell_type":"markdown","metadata":{"id":"jRDyOFywABAY","colab_type":"text"},"source":["From the basic statistics of our variable, we can see that it is asymmetric: mean is larger then the median, skewness is positive. Excess kurtosis is very large, and it may be due just to a few outliers. "]},{"cell_type":"code","metadata":{"id":"Mt4G0iEIABAb","colab_type":"code","colab":{}},"source":["print(data['Price'].describe())\n","print(scipy.stats.skew(data['Price']))\n","print(scipy.stats.kurtosis(data['Price']))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-4vU6wd-ABAe","colab_type":"text"},"source":["I make a histogram of the data, but because of outliers it doesn't show me in much details its most common part. "]},{"cell_type":"code","metadata":{"id":"x80GpS6JABAf","colab_type":"code","colab":{}},"source":["data['Price'].hist(bins=50, density=True);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTPvWT4VABAi","colab_type":"text"},"source":["Let's zoom in by plotting a histogram conditional on price being not too large.\n","\n","It still looks asymmetric. "]},{"cell_type":"code","metadata":{"id":"rB84z6fEABAj","colab_type":"code","colab":{}},"source":["data['Price'][data['Price'] <= 500].hist(bins=50, density=True);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bSTB84z9ABAn","colab_type":"text"},"source":["What kind of distribution function might describe this data? Maybe, Poisson? It's also positive and asymmetric.\n","\n","Let's generate a sample from Poisson distribution and compare its properties with the properties of our data."]},{"cell_type":"code","metadata":{"id":"Q3INAvtxABAo","colab_type":"code","colab":{}},"source":["x = pd.Series(np.random.poisson(data['Price'].mean(), size=500))\n","print(x.describe())\n","print(scipy.stats.skew(x))\n","print(scipy.stats.kurtosis(x))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BxEbMxscABAt","colab_type":"text"},"source":["Well, most of them don't coincide with the properties of our data. Median is too high; variance, skewness and kurtosis are too small.\n","\n","And if we plot the histoframs of our distributions, we'll see that they are very different. "]},{"cell_type":"code","metadata":{"id":"GdnQymufABAw","colab_type":"code","colab":{}},"source":["x.hist(density=True)\n","data['Price'].hist(density=True, bins=50, alpha=0.5)\n","plt.legend(['poisson', 'data'])\n","plt.title('histogram of Poisson distribution vs histogram of data');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dUfjPDmqABA2","colab_type":"text"},"source":["Maybe, our distributions is then lognormal?\n","\n","We could check this hypothesis by first plotting the histogram for the logarighm of our variable."]},{"cell_type":"code","metadata":{"id":"eohq2nV9ABA2","colab_type":"code","colab":{}},"source":["log_price = np.log(data['Price'])\n","log_price.hist(bins=30, density=True);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYovO2_KABA5","colab_type":"text"},"source":["Well, at least it's symmetric. \n","\n","And it's quantiles are quite close to the quantiles of normal distribution with the same location and scale."]},{"cell_type":"code","metadata":{"id":"Mx19WqSVABA6","colab_type":"code","colab":{}},"source":["log_price.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFZ6OMnwABA-","colab_type":"code","colab":{}},"source":["z = pd.Series(np.random.normal(loc=log_price.mean(), scale=log_price.std(), size=500))\n","z.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z_LOuN9aABBE","colab_type":"text"},"source":["Histograms of the two non-logarithmized distributions look similarly as well. "]},{"cell_type":"code","metadata":{"id":"hoK3lAfhABBF","colab_type":"code","colab":{}},"source":["np.exp(z).hist(bins=30, density=True)\n","data['Price'][data['Price'] <= 1000].hist(bins=30, alpha=0.5, density=True);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdRWcJJJABBH","colab_type":"text"},"source":["Finally, let's make a q-q plot to compare all the quantiles."]},{"cell_type":"code","metadata":{"id":"1OpPzEHAABBI","colab_type":"code","colab":{}},"source":["scipy.stats.probplot(\n","    data['Price'],\n","    dist=scipy.stats.lognorm(s=log_price.std(), scale=np.exp(log_price.mean()), loc=1),\n","    plot=plt\n",");"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6v-GHjXABBL","colab_type":"text"},"source":["We see that all the quantiles up to 400 align with lognormal quantiles almost perfectly, but after that, a different trend is observed. "]},{"cell_type":"markdown","metadata":{"id":"EV9YR-AbABBM","colab_type":"text"},"source":["It may mean that we in fact work with a mixture of two different distributions!\n","\n","We can think than the largest ~10 points are outliers and just ignore them. \n","\n","Or we can assume we have two different log-normal distributions, and fit them to our data (in a naive way).  "]},{"cell_type":"code","metadata":{"id":"hVnuAHulABBO","colab_type":"code","colab":{}},"source":["threshold = 350\n","\n","part1 = scipy.stats.lognorm(\n","    s=log_price[data['Price'] < threshold].std(), \n","    scale=np.exp(log_price[data['Price'] < threshold].mean())\n",")\n","part2 = scipy.stats.lognorm(\n","    s=log_price[data['Price'] >= threshold].std(), \n","    scale=np.exp(log_price[data['Price'] >= threshold].mean())\n",")\n","proportion = data['Price'] >= threshold"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X1BKLU2YABBQ","colab_type":"text"},"source":["For a mixture, there is no straigthforward way to calculate quantiles, so instead we'll just sample from it several times, and see whether these samples on average resemble the expected values."]},{"cell_type":"code","metadata":{"id":"j8wYChLPABBS","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","for i in range(30):\n","    sample = sorted(np.concatenate([\n","        part1.rvs(size=sum(data['Price'] < threshold)),\n","        part2.rvs(size=sum(data['Price'] >= threshold))\n","    ]))\n","    plt.scatter(sample, sorted(data['Price']), s=2, color='b')\n","\n","plt.plot(plt.ylim(), plt.ylim(), color='r');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRNfDUWtABBV","colab_type":"text"},"source":["Well, on average our sampled distribution almost does align with the data (however, most of the plot lies dangerously below the diagonal). \n","\n","One more good idea is to look at the same plot in log scale. "]},{"cell_type":"code","metadata":{"id":"1rMFg1RtABBW","colab_type":"code","colab":{}},"source":["np.random.seed(1)\n","for i in range(30):\n","    sample = sorted(np.concatenate([\n","        part1.rvs(size=sum(data['Price'] < threshold)),\n","        part2.rvs(size=sum(data['Price'] >= threshold))\n","    ]))\n","    plt.scatter(sample, sorted(data['Price']), s=2, color='b')\n","    \n","plt.xscale('log')\n","plt.yscale('log')\n","\n","plt.plot(plt.ylim(), plt.ylim(), color='r');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkUmYRGJABBZ","colab_type":"text"},"source":["Here it looks more okay. There is a strange group of poits at the bottom - but it corresponts to only one original observation."]},{"cell_type":"code","metadata":{"id":"LhveIovxABBa","colab_type":"code","colab":{}},"source":["data['Price'].sort_values().head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rfphI3POABBc","colab_type":"text"},"source":["By the way, stock prices seem to depend a lot on the sector. We can visualize it, but we won't go deeper here. "]},{"cell_type":"code","metadata":{"id":"E7FF6AY2ABBd","colab_type":"code","colab":{}},"source":["data.boxplot(column='Price', by='Sector', figsize=(6, 6), vert=False);\n","plt.xscale('log');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5PpS1YH5ABBf","colab_type":"text"},"source":["# 5. The assignment"]},{"cell_type":"markdown","metadata":{"id":"p2GPYjuRABBg","colab_type":"text"},"source":["Take two other variables from the same dataset, `EBITDA` and `Market Cap`. For each of them, \n","* calculate descriptive statsistics and comment on them;\n","* try different ways of visualizing the variable;\n","* try to fit a 'theoretical' distribution for this variable. \n","\n","There is no single \"right\" way to do this assignment. \n","\n","But the more interesting observations you will make about the two variables, the better it would be."]},{"cell_type":"code","metadata":{"id":"3jUrgTu2JxUw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}